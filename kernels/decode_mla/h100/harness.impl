#include <iostream>
#include <string>
#include <fstream>

constexpr int ATTN_B     = 4;   // regen input via gentests

constexpr int QO_HEADS   = 4;   // regen input via gentests
constexpr int KV_HEADS   = 4;   // regen input via gentests

constexpr int Q_LEN      = 1;   // regen input via gentests
constexpr int L_NEW      = 1;   // regen input via gentests

constexpr int ATTN_N     = 768; // regen input via gentests
constexpr int ATTN_D     = 128; // regen input via gentests

constexpr int QK_HEAD_RATIO = (QO_HEADS)/(KV_HEADS);
static_assert(QO_HEADS >= KV_HEADS && QO_HEADS % KV_HEADS == 0);
static_assert(QK_HEAD_RATIO == 1); // for now

constexpr int BLOCK_SIZE = (32*NUM_WORKERS); 
constexpr int ITER       = 10; 

constexpr bool causal = false; 

#define CudaCheckError()    __cudaCheckError( __FILE__, __LINE__ )
inline void __cudaCheckError( const char *file, const int line ) {
    cudaError err = cudaGetLastError();
    if ( cudaSuccess != err )
    {
        fprintf( stderr, "cudaCheckError() failed at %s:%i : %s\n",
                 file, line, cudaGetErrorString( err ) );
        exit( -1 );
    }
    // More careful checking. However, this will affect performance.
    // Comment away if needed.
    err = cudaDeviceSynchronize();
    if( cudaSuccess != err )
    {
        fprintf( stderr, "cudaCheckError() with sync failed at %s:%i : %s\n",
                 file, line, cudaGetErrorString( err ) );
        exit( -1 );
    }
}

// Function to calculate the number of floating-point operations
long long flops(int batch, int seqlen, int headdim, int nheads, bool causal, const std::string& mode) {
    assert(mode == "fwd" || mode == "bwd" || mode == "fwd_bwd");
    long long f = 4 * batch * static_cast<long long>(seqlen) * seqlen * nheads * headdim;
    f /= (causal ? 2 : 1);

    if (mode == "fwd") {
        return f;
    } else if (mode == "bwd") {
        return static_cast<long long>(2.5 * f);
    } else { // fwd_bwd
        return static_cast<long long>(3.5 * f);
    }
}

// Function to calculate the efficiency in teraflops
double efficiency(long long flop, double time) {
    // Convert flop to teraflops and time to milliseconds
    double tflops = flop / 1e12;
    double time_ms = time / 1e6;
    return tflops / time_ms;
}

int main(int argc, char **argv) {
    std::cout << "Entered main!" << std::endl;

    // queries
    constexpr int TOTAL_Q_ELEMENTS        = ATTN_B*QO_HEADS*Q_LEN*ATTN_D;
    constexpr int TOTAL_Q_UNIQUE_ELEMENTS = Q_LEN*ATTN_D;
    float *q   = new float[TOTAL_Q_UNIQUE_ELEMENTS];
    bf16 *q_bf = new bf16[TOTAL_Q_ELEMENTS];

    // keys and values cahces
    constexpr int TOTAL_KV_CACHE_ELEMENTS        = ATTN_B*KV_HEADS*ATTN_N*ATTN_D;
    constexpr int TOTAL_KV_CACHE_UNIQUE_ELEMENTS = ATTN_N*ATTN_D;
    float *k_cache   = new float[TOTAL_KV_CACHE_UNIQUE_ELEMENTS];
    float *v_cache   = new float[TOTAL_KV_CACHE_UNIQUE_ELEMENTS];
    bf16 *k_cache_bf = new  bf16[TOTAL_KV_CACHE_ELEMENTS];
    bf16 *v_cache_bf = new  bf16[TOTAL_KV_CACHE_ELEMENTS];

    // keys and values new
    constexpr int TOTAL_KV_NEW_ELEMENTS        = ATTN_B*KV_HEADS*L_NEW*ATTN_D;
    constexpr int TOTAL_KV_NEW_UNIQUE_ELEMENTS = L_NEW*ATTN_D;
    float *k_new    = new float[TOTAL_KV_NEW_UNIQUE_ELEMENTS];
    float *v_new    = new float[TOTAL_KV_NEW_UNIQUE_ELEMENTS];
    bf16  *k_new_bf = new  bf16[TOTAL_KV_NEW_ELEMENTS];
    bf16  *v_new_bf = new  bf16[TOTAL_KV_NEW_ELEMENTS];

    // keys and values seqlens
    constexpr int TOTAL_KV_SEQLEN_ELEMENTS = ATTN_B;
    float *k_seqlens   = new float[TOTAL_KV_SEQLEN_ELEMENTS];
    bf16 *k_seqlens_bf = new  bf16[TOTAL_KV_SEQLEN_ELEMENTS];

    // output
    constexpr int TOTAL_OUT_ELEMENTS        = ATTN_B*QO_HEADS*Q_LEN*ATTN_D;
    constexpr int TOTAL_OUT_UNIQUE_ELEMENTS = Q_LEN*ATTN_D;
    float *o_ref = new float[TOTAL_OUT_UNIQUE_ELEMENTS];
    bf16 *o_bf   = new  bf16[TOTAL_OUT_ELEMENTS];

    float *o = new float[TOTAL_OUT_ELEMENTS];

    std::ifstream infile(argv[1]);

    std::cout << "Starting to enter!" << std::endl;

    for(int i = 0; i < TOTAL_Q_UNIQUE_ELEMENTS; i++) infile >> q[i];
    std::cout << "Finished loading Q" << std::endl;
    for(int i = 0; i < TOTAL_KV_CACHE_UNIQUE_ELEMENTS; i++) infile >> k_cache[i];
    std::cout << "Finished loading K CACHE" << std::endl;
    for(int i = 0; i < TOTAL_KV_CACHE_UNIQUE_ELEMENTS; i++) infile >> v_cache[i];
    std::cout << "Finished loading V CACHE" << std::endl;
    for(int i = 0; i < TOTAL_KV_NEW_UNIQUE_ELEMENTS; i++) infile >> k_new[i];
    std::cout << "Finished loading K NEW" << std::endl;
    for(int i = 0; i < TOTAL_KV_NEW_UNIQUE_ELEMENTS; i++) infile >> v_new[i];
    std::cout << "Finished loading V NEW" << std::endl;
    for(int i = 0; i < TOTAL_KV_SEQLEN_ELEMENTS; i++) infile >> k_seqlens[i];
    std::cout << "Finished loading K SEQLENS" << std::endl;
    for(int i = 0; i < TOTAL_OUT_UNIQUE_ELEMENTS; i++) infile >> o_ref[i];
    std::cout << "Finished loading O REF" << std::endl;

    std::cout << "Finished loading file from " << argv[1] << "!" << std::endl;
    
    // replicate into batch
    for(int i = 0; i < TOTAL_Q_ELEMENTS; i++) {
        q_bf[i] = __float2bfloat16(q[i % TOTAL_Q_UNIQUE_ELEMENTS]);
    }
    for(int i = 0; i < TOTAL_KV_CACHE_ELEMENTS; i++) {
        k_cache_bf[i] = __float2bfloat16(k_cache[i % TOTAL_KV_CACHE_UNIQUE_ELEMENTS]);
        v_cache_bf[i] = __float2bfloat16(v_cache[i % TOTAL_KV_CACHE_UNIQUE_ELEMENTS]);
    }
    for(int i = 0; i < TOTAL_KV_NEW_ELEMENTS; i++) {
        k_new_bf[i] = __float2bfloat16(k_new[i % TOTAL_KV_NEW_UNIQUE_ELEMENTS]);
        v_new_bf[i] = __float2bfloat16(v_new[i % TOTAL_KV_NEW_UNIQUE_ELEMENTS]);
    }
    for(int i = 0; i < TOTAL_KV_SEQLEN_ELEMENTS; i++) {
        k_seqlens_bf[i] = __float2bfloat16(k_seqlens[i]);
    }

    bf16 *d_q, *d_k_cache, *d_v_cache, *d_k_new, *d_v_new, *d_k_seqlens, *d_o;
    cudaMalloc(&d_q,         TOTAL_Q_ELEMENTS         * sizeof(bf16));
    cudaMalloc(&d_k_cache,   TOTAL_KV_CACHE_ELEMENTS  * sizeof(bf16));
    cudaMalloc(&d_v_cache,   TOTAL_KV_CACHE_ELEMENTS  * sizeof(bf16));
    cudaMalloc(&d_k_new,     TOTAL_KV_NEW_ELEMENTS    * sizeof(bf16));
    cudaMalloc(&d_v_new,     TOTAL_KV_NEW_ELEMENTS    * sizeof(bf16));
    cudaMalloc(&d_k_seqlens, TOTAL_KV_SEQLEN_ELEMENTS * sizeof(bf16));
    cudaMalloc(&d_o,         TOTAL_OUT_ELEMENTS       * sizeof(bf16));

    cudaMemcpy(d_q,         q_bf,         TOTAL_Q_ELEMENTS         * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_k_cache,   k_cache_bf,   TOTAL_KV_CACHE_ELEMENTS  * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_v_cache,   v_cache_bf,   TOTAL_KV_CACHE_ELEMENTS  * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_k_new,     k_new_bf,     TOTAL_KV_NEW_ELEMENTS    * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_v_new,     v_new_bf,     TOTAL_KV_NEW_ELEMENTS    * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_k_seqlens, k_seqlens_bf, TOTAL_KV_SEQLEN_ELEMENTS * sizeof(bf16), cudaMemcpyHostToDevice);

    using q_tile         = st_bf<fwd_attend_ker_tile_dims<ATTN_D>::new_height,   fwd_attend_ker_tile_dims<ATTN_D>::tile_width>;
    using k_cache_tile   = st_bf<fwd_attend_ker_tile_dims<ATTN_D>::cache_height, fwd_attend_ker_tile_dims<ATTN_D>::tile_width>;
    using v_cache_tile   = st_bf<fwd_attend_ker_tile_dims<ATTN_D>::cache_height, fwd_attend_ker_tile_dims<ATTN_D>::tile_width>;
    using k_new_tile     = st_bf<fwd_attend_ker_tile_dims<ATTN_D>::new_height,   fwd_attend_ker_tile_dims<ATTN_D>::tile_width>;
    using v_new_tile     = st_bf<fwd_attend_ker_tile_dims<ATTN_D>::new_height,   fwd_attend_ker_tile_dims<ATTN_D>::tile_width>;
    using k_seqlens_tile = st_bf<fwd_attend_ker_tile_dims<ATTN_D>::new_height,   fwd_attend_ker_tile_dims<ATTN_D>::tile_width>;
    using o_tile         = st_bf<fwd_attend_ker_tile_dims<ATTN_D>::new_height,   fwd_attend_ker_tile_dims<ATTN_D>::tile_width>;

    using q_global         = gl<bf16,  -1, -1, -1, -1, q_tile>;
    using k_cache_global   = gl<bf16,  -1, -1, -1, -1, k_cache_tile>;
    using v_cache_global   = gl<bf16,  -1, -1, -1, -1, v_cache_tile>;
    using k_new_global     = gl<bf16,  -1, -1, -1, -1, k_new_tile>;
    using v_new_global     = gl<bf16,  -1, -1, -1, -1, v_new_tile>;
    using k_seqlens_global = gl<bf16,  -1, -1, -1, -1, k_seqlens_tile>;
    using o_global         = gl<bf16,  -1, -1, -1, -1, o_tile>;

    using globals          = fwd_globals<ATTN_D>;

    q_global         qg_arg{d_q,         ATTN_B, QO_HEADS, Q_LEN,  ATTN_D};
    k_cache_global   kc_arg{d_k_cache,   ATTN_B, KV_HEADS, ATTN_N, ATTN_D};
    v_cache_global   vc_arg{d_v_cache,   ATTN_B, KV_HEADS, ATTN_N, ATTN_D};
    k_new_global     kn_arg{d_k_new,     ATTN_B, KV_HEADS, Q_LEN,  ATTN_D};
    v_new_global     vn_arg{d_v_new,     ATTN_B, KV_HEADS, Q_LEN,  ATTN_D};
    k_seqlens_global ks_arg{d_k_seqlens, ATTN_B,        1,     1,       1};
    o_global         og_arg{d_o,         ATTN_B, QO_HEADS, Q_LEN,  ATTN_D};

    globals g{qg_arg, kc_arg, vc_arg, 
                      kn_arg, vn_arg, 
                      ks_arg, og_arg, 
                      ATTN_N, QK_HEAD_RATIO};

    std::cout << "Allocated and set memory on GPU!" << std::endl;
    
    unsigned long mem_size = kittens::MAX_SHARED_MEMORY; // need to launch two blocks if possible.
    
    cudaFuncSetAttribute(
        fwd_attend_ker<ATTN_D, causal>,
        cudaFuncAttributeMaxDynamicSharedMemorySize,
        mem_size
    );

    std::cout << "Set max dynamic memory!" << std::endl;

    // prefix sum over the vector
    int kv_split_blocks = (ATTN_N + KV_SPLIT - 1) / KV_SPLIT;

    // dim3 grid(ATTN_N/(CONSUMER_WARPGROUPS*kittens::TILE_ROW_DIM<bf16>*4), QO_HEADS, ATTN_B);
    int kv_split_blocks = (ATTN_N + KV_SPLIT - 1) / KV_SPLIT;
    dim3 grid(kv_split_blocks, QO_HEADS, ATTN_B);
    cudaDeviceSynchronize();
    std::cout << "Starting warmup" << std::endl;
    for(int i = 0; i < ITER; i++) {    
        fwd_attend_ker<ATTN_D, causal><<<grid, BLOCK_SIZE, mem_size>>>(g);
    }
    cudaDeviceSynchronize();
    std::cout << "Starting kernel" << std::endl;
    const auto start = std::chrono::high_resolution_clock::now();
    for(int i = 0; i < ITER; i++) {
        fwd_attend_ker<ATTN_D, causal><<<grid, BLOCK_SIZE, mem_size>>>(g);
    }
    cudaDeviceSynchronize();
    const auto finish = std::chrono::high_resolution_clock::now();
    CudaCheckError();
    std::cout << "Finished kernel\n";

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        std::cout << "CUDA error: " << cudaGetErrorString(err) << std::endl;
        return 1;
    }

    /*******************************
     * CORRECTNESS 
    ********************************* */
    
    // check correctness
    cudaMemcpy(o_bf, d_o, TOTAL_OUT_ELEMENTS * sizeof(bf16), cudaMemcpyDeviceToHost);
    for(int i = 0; i < TOTAL_OUT_UNIQUE_ELEMENTS; i++) {
        o[i] = __bfloat162float(o_bf[i]);
    }

    bool good = true;
    std::ofstream o_ref_file("printouts/o_ref.txt");
    std::ofstream o_file("printouts/o.txt");
    std::ofstream diff_file("printouts/diff.txt");
    for(int i = 0; i < TOTAL_OUT_UNIQUE_ELEMENTS; i++) {
        float diff = o[i] - o_ref[i % TOTAL_OUT_UNIQUE_ELEMENTS];
        o_ref_file << o_ref[i % TOTAL_OUT_UNIQUE_ELEMENTS] << ' ';
        o_file << o[i] << ' ';
        diff_file << diff << ' ';
        if(i % 64 == 63) {
            o_ref_file << '\n';
            o_file << '\n';
            diff_file << '\n';
        }
        if(abs(diff) > 0.01 || isnan(diff)) {
            good = false;
        }
    }

    std::cout << "Average execution time: " << std::chrono::duration_cast<std::chrono::microseconds>(finish - start).count() / ITER << " us" << std::endl;
    if(good) std::cout << "Correct :)\n";
    else     std::cout << "Incorrect :(\n";
    
    // Compute and print average time
    double avg_time_s = (double)(std::chrono::duration_cast<std::chrono::microseconds>(finish - start).count()) / (ITER * 1e6);
    std::cout << "Time: " << avg_time_s << " s" << std::endl;

    cudaFree(d_q);
    cudaFree(d_k_cache);
    cudaFree(d_v_cache);
    cudaFree(d_k_new);
    cudaFree(d_v_new);
    cudaFree(d_k_seqlens);
    cudaFree(d_o);

    delete[] q, k_cache, v_cache, k_new, v_new, k_seqlens, o_ref, o;
    delete[] q_bf, k_cache_bf, v_cache_bf, k_new_bf, v_new_bf, k_seqlens_bf, o_bf;

    return 0;
}
